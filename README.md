-->Data-to-Insight System

This project is part of my internship assignment.
The goal was to build an end-to-end data pipeline that takes raw data, processes it, and makes it searchable through an API.

I used Spark for ETL, Parquet as storage, TF-IDF for embeddings, Qdrant for vector search, and FastAPI to expose the results.

ğŸ“Œ What this project does

Reads raw NYC taxi data (CSV)

Cleans and transforms the data using Spark

Stores processed data in Silver and Gold layers

Converts Gold data into text and embeddings

Stores embeddings in a vector database (Qdrant)

Allows users to query insights using a FastAPI service

Example question:

â€œWhat was the average taxi fare?â€

ğŸ—ï¸ Overall Flow
Raw CSV
  â†“
Spark ETL
  â†“
Silver Parquet
  â†“
Gold Parquet (KPIs)
  â†“
TF-IDF Embeddings
  â†“
Qdrant
  â†“
FastAPI Search API

ğŸ§° Technologies Used

Apache Spark (PySpark) â€“ data processing

Parquet â€“ data lake storage

scikit-learn (TF-IDF) â€“ text vectorization

Qdrant â€“ vector database

FastAPI â€“ REST API

Docker & Docker Compose â€“ containerization

ğŸ“‚ Project Structure
project-root/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”‚
â”œâ”€â”€ spark-apps/
â”‚   â””â”€â”€ etl.py
â”‚
â”œâ”€â”€ vector/
â”‚   â”œâ”€â”€ embed_and_index.py
â”‚   â””â”€â”€ tfidf_model.pkl
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ main.py

ğŸ”„ Data Pipeline Explanation
1ï¸âƒ£ ETL using Spark (etl.py)

Loads raw taxi CSV data

Cleans columns like fare and pickup time

Creates:

Silver layer â†’ cleaned data

Gold layer â†’ aggregated KPIs like average fare and total trips

2ï¸âƒ£ Embedding & Indexing (embed_and_index.py)

Reads data from the Gold layer

Converts each row into a text summary

Uses TF-IDF to create vectors

Saves the trained model as tfidf_model.pkl

Indexes vectors into Qdrant

This step runs once before starting the API.

3ï¸âƒ£ FastAPI Service (main.py)

Loads the saved TF-IDF model at startup

Connects to Qdrant

Exposes endpoints to search KPIs using natural language

ğŸŒ API Endpoints
Health Check
GET /


Returns the current system status and shows whether:

model is loaded

Qdrant is connected

Search KPIs
GET /search?query=average fare


Example response:

{
  "results": [
    {
      "score": 0.82,
      "text": "On 2016-01-01, the average taxi fare was 12.45 dollars with 45678 trips."
    }
  ]
}

ğŸ³ How to Run the Project
1ï¸âƒ£ Start required services
docker compose up -d qdrant spark

2ï¸âƒ£ Run Spark ETL
docker exec -it spark python3 /opt/spark-apps/etl.py

3ï¸âƒ£ Run embedding and indexing (one time)
docker exec -it spark python3 /vector/embed_and_index.py


This generates the TF-IDF model file.

4ï¸âƒ£ Start FastAPI
docker compose up -d fastapi

5ï¸âƒ£ Test the API
curl "http://localhost:8000/search?query=average fare"

ğŸ§  Why I chose this approach

Used TF-IDF instead of heavy transformer models to keep the system lightweight

Separated offline indexing from online querying

Used Qdrant to support semantic similarity instead of normal SQL search

Designed the pipeline similar to real production systems

ğŸš€ Future Improvements

Add Airflow for scheduling

Add MinIO for object storage

Add LLM-based answer generation

Add caching for repeated queries

Improve monitoring and logging

ğŸ“Œ What I learned

Building ETL pipelines with Spark

Working with data lake layers

Using vector databases for semantic search

Containerizing multi-service systems

Debugging real-world data engineering issues